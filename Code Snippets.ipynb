{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02554f59",
   "metadata": {},
   "source": [
    "# READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('file.csv')\n",
    "df = pd.read_excel('file.xlsx')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "pd.concat([df,df],axis=1) # columnas\n",
    "\n",
    "df.info()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af5f03",
   "metadata": {},
   "source": [
    "# SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5801d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_SQL\n",
    "pd.to_csv\n",
    "df.to_csv('name.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ae230",
   "metadata": {},
   "source": [
    "# Convert type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95394df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = df['col'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f1002",
   "metadata": {},
   "source": [
    "# Changin values mapping column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal(df,col,names):\n",
    "    return df[col].map({names[0]:0, names[1]:1, names[2]:2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5443f",
   "metadata": {},
   "source": [
    "# About COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ba4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save changes, we have to = to a df name, the same or a different one\n",
    "\n",
    "# slicing one column\n",
    "df[\"col\"] #it gives a pandas series\n",
    "\n",
    "\n",
    "\n",
    "#Slicing one column as a DataFrame, in stead of a Series\n",
    "df[['col']]\n",
    "\n",
    "\n",
    "\n",
    "#Slicing multiple columns\n",
    "df[['col','col','col']]\n",
    "\n",
    "\n",
    "# checking duplicates\n",
    "df.duplicated(keep='first')\n",
    "\n",
    "\n",
    "#reordering columns\n",
    "df[['col3','col1','col5']]\n",
    "\n",
    "\n",
    "\n",
    "#some useful column methods\n",
    "# unique method of pandas \n",
    "df['col'].nunique()                     # COUNT(DISTINCT col)\n",
    "\n",
    "df['col'].unique()                      # DISTINCT col\n",
    "\n",
    "df['col'].value_counts()                # COUNT(*) + GROUP BY col\n",
    "\n",
    "df['col'].value_counts(dropna=False)    #COUNT(*) + GROUP BY col and shows NaNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e6e4b",
   "metadata": {},
   "source": [
    "# About COLUMN AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can do computations over columns\n",
    "df['col']/df['col']\n",
    "\n",
    "\n",
    "\n",
    "#new columns from previous ones\n",
    "df['new_col'] = df['col']/df['col']\n",
    "\n",
    "\n",
    "\n",
    "# highest value\n",
    "df['col'].max()  # SELECT MAX(col)\n",
    "\n",
    "df.max() #max value of each column, in categoricals, the max alfabetically\n",
    "\n",
    "\n",
    "\n",
    "# average col value, it gaves the mean of each column you use\n",
    "\n",
    "\n",
    "df.mean() # only on the columns where it makes sense, numerical ones\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01652e8",
   "metadata": {},
   "source": [
    "# Filtering & BOOLEAN Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a column only True/False is called a Boolean Mask\n",
    "#Boolean masks are very useful to filter rows in DataFrames\n",
    "df[df['col']>100].head() # WHERE col > 100 LIMIT 5\n",
    "\n",
    "\n",
    "df[(df['col1']=='value1')&(df['col2']=='value2')] #this gives a filtered data where col1 have value1 AND col2 have value2\n",
    "\n",
    "\n",
    "\n",
    "# find the row with the maximum col\n",
    "df[df['col'] == df['col'].max()]  #this gives the ROW where col have it's max value\n",
    "\n",
    "\n",
    "\n",
    "narrowdown_states = data[(data['STATE']== 'CA')|(data['STATE'] == 'IL')]\n",
    "\n",
    "# & : AND\n",
    "# | : OR\n",
    "# ~ : NOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90639e15",
   "metadata": {},
   "source": [
    "# About SORTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99840f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='col1', ascending=False, na_position='first') # last 2 might not be needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c201d18",
   "metadata": {},
   "source": [
    "# About DROPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "df.drop(['col', 'col'], axis=1)\n",
    "\n",
    "\n",
    "#drop rows\n",
    "df = df[~df['col'].isin(['value', 'value', 'value'])] #here we drop the rows with that values in that column from df\n",
    "# we keep the info that IS NOT IN those that's the meaning of ~ = is not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b83f4",
   "metadata": {
    "id": "qgoEVptK0bNR"
   },
   "source": [
    "# BINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing a numeric variable into a categorical one ('low', 'medium', 'high')\n",
    "# how to perform binning -> bins on a variable\n",
    "# cut in 5 equally SPACED bins with this (this is an example) \n",
    "# cut calculates the minimun value and the maximun and it divides it into spaces with the same width\n",
    "names = [\"Very Low\", \"Low\", \"Moderate\", \"High\", \"Very High\"]  #they are used in increasing order\n",
    "bins = pd.cut(df['col'],5, labels=names)\n",
    "\n",
    "\n",
    "\n",
    "# qcut in 5 equally FILLED (%) bins\n",
    "qbins = pd.qcut(df['col'],5, labels = names)\n",
    "\n",
    "\n",
    "\n",
    "# cut in 5 DESIGNED bins\n",
    "bins = pd.cut(df['col'],[0,10,100,1000,2000,8000], labels = names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c4ac7",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b112385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by similar to SQL, but with a lot of flexibility\n",
    "# groupby(the dimensions you want to group by).agg({measure(s) to aggregate: aggegation function(s)})\n",
    "# some examples:\n",
    "\n",
    "df.groupby(['col']).agg({'col':sum,'col':min})\n",
    "\n",
    "\n",
    "\n",
    "df.groupby(['col1','col2']).agg({'col3':sum,'col4':min,'col5':pd.Series.nunique})\n",
    "#this made col1 and col2 as 2 indexes, they are no longer columns but\n",
    "#Multi-indexes can be hard to manage\n",
    "#Tip, you can turn the index back into columns by using reset_index()\n",
    "df.groupby(['col1','col2']).agg({'col3':sum,'col4':min,'col5':pd.Series.nunique}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe6c02",
   "metadata": {},
   "source": [
    "# About NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing only na values\n",
    "\n",
    "df.isna().sum()\n",
    "df.isna().any()\n",
    "df.isna().any(axis=1)\n",
    "df[df.columns[df.isna().any()]]\n",
    "\n",
    "df['col'].isna().sum() # how many NaN in that col\n",
    "\n",
    "numericals['INCOME'].fillna(data['INCOME'].mean()) # fill na with mean\n",
    "\n",
    "numericals_with_income = numericals[numericals['INCOME'].notna()] #we take the rows where incom is not na\n",
    "\n",
    "data['final_income'] =  np.where(data['INCOME'].isna(), data['predicted_income'], data['INCOME']) # fill na with values from other column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f86bd6",
   "metadata": {},
   "source": [
    "# Applying FUNCTIONS  L3.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(function,df['value']))\n",
    "\n",
    "function(variables)\n",
    "\n",
    "df['value'].apply(function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74b12f",
   "metadata": {},
   "source": [
    "# MERGING L3.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left = df1,\n",
    "        right = df2,\n",
    "        how = 'inner', \n",
    "        left_on = \"col_df1\", \n",
    "        right_on= \"col_df2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad09ff5",
   "metadata": {},
   "source": [
    "# MELTING - making vertical L3.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb92924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.melt( df to melt, columns to keep)\n",
    "verticalized = pd.melt(df,id_vars=['col']) #it gaves everything verticalized\n",
    "\n",
    "verticalized[verticalized['col']== 'value'] # it gives the values verticalized from de 'value' we call\n",
    "\n",
    "verticalized[verticalized['col of cols'].isin(['value','value'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3db6bc",
   "metadata": {},
   "source": [
    "# PIVOT - making horizontal L3.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed134c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot(index=['first_col','second_col'],columns='col_that_become_names_of_the_new_columns')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# using an aggregate column index:\n",
    "df[[('col1', ''),('col_from_cols','col'),('col_from_cols','col'),('col_from_cols','col')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9639c062",
   "metadata": {},
   "source": [
    "# PIVOT TABLE - tablas dinámicas L3.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2949d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT aggregation is the AVERAGE\n",
    "\n",
    "\n",
    "df.pivot_table(index= ['col']) \n",
    "# pandas did a group by Category\n",
    "# it kept only the numeric columns\n",
    "# then aggregated the results for those with the mean, which is his default aggregation\n",
    "\n",
    "\n",
    "df.pivot_table(index= ['col1'],columns=['col2'])\n",
    "# in this case it does not get the mean, it uses the information for col2\n",
    "\n",
    "\n",
    "\n",
    "# we can do the same pivot all in the rows\n",
    "df.pivot_table(index= ['col1','col2'])\n",
    "\n",
    "\n",
    "\n",
    "# index = name of rows  columns = name of columns\n",
    "# we can get rid of the index with .reset_index() the same as before\n",
    "\n",
    "\n",
    "# and you can pick one or more aggregations\n",
    "df.pivot_table(index= ['col1','col2'], values = ['col3','col4'], aggfunc = ['sum','mean'])\n",
    "\n",
    "df.pivot_table(index= ['col1','col2'], values = ['col3','col4'], aggfunc = {'col3':'sum','col4':'mean'}).reset_index()\n",
    "\n",
    "df.columns = ['Category','Region', 'MeanProfit', 'TotalSales'] # new names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f769ab",
   "metadata": {},
   "source": [
    "# CrossTAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['col'], df['col'])\n",
    "pd.crosstab(df.col, df.col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b8d8a",
   "metadata": {},
   "source": [
    "# Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba851ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transfom(x):\n",
    "    x = np.log10(x)\n",
    "    if np.isfinite(x):\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "df['col'].apply(np.log)  # another way of log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06dab00",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.distplot(df['col'])    # here they appear together in the same area\n",
    "sns.distplot(df['col'])\n",
    "plt.show()\n",
    "\n",
    "sns.displot(df['col'], kde=True)  #here it does not give error\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x=col, y=col, data=df)\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x = col, y=col, data=df)\n",
    "plt.show()\n",
    "\n",
    "df[[col]].boxplot()\n",
    "plt.show()\n",
    "\n",
    "df[col].hist(bins=100)    #histogram\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x=df['col'], y=df['col'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.pairplot(df)  #it gives plots for everything together\n",
    "\n",
    "sns.countplot(x=df[col])   # bar plots colours  \n",
    "plt.show()   \n",
    "\n",
    "\n",
    "plt.xlabel(df[col].name)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))   ## FIGSIZE!!\n",
    "plt.plot(range(2,25),scores,color = 'blue', linestyle='dashed',\n",
    "         marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('R2-scores vs. K Value')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# check this, two plots side by side\n",
    "\n",
    "options, charts = plt.subplots(1, 2, figsize=(12, 6))\n",
    "colors = np.array(['blue', 'red'])\n",
    "charts[0].scatter(X[:, 0], X[:, 1], color=colors[y1_pred])\n",
    "charts[1].scatter(X[:, 0], X[:, 1], color=colors[y2_pred])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fee0e",
   "metadata": {},
   "source": [
    "# HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df, annot=True) # annot = annotation, print the values inside the squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd6211",
   "metadata": {},
   "source": [
    "# Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 = 20%, 42 not needed, it's just for keeping the same random piece each time you run it\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "X_train_num = X_train.select_dtypes(include = np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90851ee7",
   "metadata": {},
   "source": [
    "# Normilizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MinMaxtransformer = MinMaxScaler().fit(X_num) #here we train the transformer\n",
    "X_normalized = MinMaxtransformer.transform(X_num) #this is an nd array, nd means n dimensional, 2 dimensions, it has width and length\n",
    "print(X_normalized.shape)\n",
    "X_normalized = pd.DataFrame(X_normalized,columns=X_num.columns) #here we convert it into a DataFrame\n",
    "X_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34647e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "Standardtransformer = StandardScaler().fit(X_num)\n",
    "X_standardized = Standardtransformer.transform(X_num)\n",
    "print(X_standardized.shape)\n",
    "X_standardized = pd.DataFrame(X_standardized,columns=X_num.columns)\n",
    "X_standardized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a7a1f",
   "metadata": {},
   "source": [
    "# One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00083ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='error', drop='first').fit(categoricals_train) \n",
    "# if there is new categories in nexts transforming data, it shows an error\n",
    "\n",
    "categoricals_train_encoded = encoder.transform(categoricals_train).toarray()\n",
    "categoricals_test_encoded = encoder.transform(categoricals_test).toarray()\n",
    "\n",
    "encoder.categories_\n",
    "\n",
    "\n",
    "X_train = np.concatenate((numericals_train_standardized,categoricals_train_encoded),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(X_train)\n",
    "# --------------------------------------#\n",
    "\n",
    "encoder = OneHotEncoder(drop='first').fit(X_cat)\n",
    "# print(encoder.categories_)\n",
    "transformer = encoder.transform(X_cat).toarray()\n",
    "# print(encoded)\n",
    "\n",
    "\n",
    "\n",
    "cols = encoder.get_features_names(input_features=X_test_cat.columns)\n",
    "\n",
    "X_test_cat_encoded = pd.DataFrame(encoder.transfoirm(X_test_cat).toarray(),colmns=cols)\n",
    "\n",
    "onehot_encoded = pd.DataFrame(transformer,columns=cols)\n",
    "onehot_encoded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbfd32",
   "metadata": {},
   "source": [
    "# Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe822db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X_test_categorical, columns=['type', 'operation', 'k_symbol', 'duration'], drop_first=True)\n",
    "\n",
    "\n",
    "# it does the same as one hot encoder, but you have to check the order when train-test, \n",
    "# because sometimes it does not give the same columns order\n",
    "\n",
    "# verify that dummies columns are in the same order and that the same column was dropped\n",
    "list(zip(list(X_train_cat.columns),list(X_test_cat.columns)))\n",
    "# not needed if you treat each dataframe with one_hot_encoder and save the encode (and the column names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320b0d8",
   "metadata": {},
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it gives numbers to the variables, BUT it may affect, because there are numbers higher than others and maybe the data is not supposed to be grater\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoded = LabelEncoder().fit(X_cat).transform(X_cat) # ordered wrt value counts\n",
    "label_encoded = pd.DataFrame(label_encoded,columns=X_cat.columns)\n",
    "display(label_encoded.head(20))\n",
    "label_encoded['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f20ff2",
   "metadata": {},
   "source": [
    "# Check for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix=data.corr(method='pearson')  # default\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax = sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d75b610",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a4f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f89721",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "lm_right.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lm_right.predict(X_train)\n",
    "r2_score(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86819340",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test1 = lm_right.predict(new_data_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_added_constant = sm.add_constant(X)   # needs a constant column with value 1.0\n",
    "X_added_constant  \n",
    "model = sm.OLS(y,X_added_constant).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0be213",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b07220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train_transformed, y_train)\n",
    "\n",
    "LR = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "LR.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc779fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.score(X_test_scaled, y_test)\n",
    "\n",
    "#while accuracy is not absolutely terrible, a closer look reveals some serious problems\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pred = LR.predict(X_test_scaled)\n",
    "\n",
    "print(\"precision: \",precision_score(y_test,pred))\n",
    "print(\"recall: \",recall_score(y_test,pred))\n",
    "print(\"f1: \",f1_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9be0d7",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f16229",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, predictions_test)\n",
    "rmse = math.sqrt(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d09a0",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)\n",
    "\n",
    "# predicted | A | B | C | D |\n",
    "# --------------------------\n",
    "# actual  A | + |  |   |   |\n",
    "# --------------------------\n",
    "#         B |   | + |   |   |\n",
    "# --------------------------\n",
    "#         C |   |   | + |   |\n",
    "# --------------------------\n",
    "#         D |   |   |   | + |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db1e92",
   "metadata": {},
   "source": [
    "# KNN classifier : neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9793413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN classifier: look at nearest neighbours and use the majority to determine class\n",
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "clf.fit(X_train_transformed, y_train)\n",
    "predictions_clf = clf.predict(X_test_transformed)\n",
    "clf.score(X_test_transformed, y_test)\n",
    "\n",
    "\n",
    "predicted_income = pd.Series(KNN.predict(X_normalized_all),name='predicted_income')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db418723",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.corr(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac531cb",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feca7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "text = 'a text about anything'\n",
    "\n",
    "pattern = 'er'    # it gives a list of 'er' the times it is in the text\n",
    "\n",
    "pattern = '[er]' # it gives all e and all r in the text as separated characters, but not both er\n",
    "\n",
    "pattern = 'gr[ae]y' #  it gives all of the gray or grey but not both like graey\n",
    "\n",
    "pattern = '[A-Z]' # all characters between uppercase and also A and Z\n",
    "\n",
    "pattern = '[1-3]' # all numbers 1, 2, 3\n",
    " # \\ takes away the special meaning of a character\n",
    "pattern = '[1\\-3]' # that gives the 1, the - and the 3\n",
    "\n",
    "\n",
    "   # META CHARACTERS:\n",
    "# \\n new line--> this makes a new line inside a string\n",
    "# [] Match set of characters\n",
    "# . Match any character except new line (it gives a list of separate characters)\n",
    "# ^  1. MAtch characters not listed if within set or 2. match beggining of string\n",
    "# $ Match end of string\n",
    "# | Function as an 'OR' operator\n",
    "\n",
    "pattern = '[^a-m]' #it in a set [] so it gives everything that is not between a-m\n",
    "pattern = '^My boss'  #it is not on a set, so it gives My boss as a string if it is in the beggining of a string\n",
    "\n",
    "pattern = '[Mm]y' # it gives all 'My' and 'my' as list\n",
    "\n",
    "pattern = 'they are not.$' # it gives the string if it is at the end of a string\n",
    "\n",
    "pattern = '[Mm]y | TPS |reports' # it gives a list of each time appears each of those strings\n",
    "\n",
    "\n",
    "   # SPECIFY NUMBER OF MATCHES:\n",
    "# *: Matches previous character 0 or more times\n",
    "# +: Matches previous character 1 or more times\n",
    "# ?: Matches previous character 0 or 1 times (optional)\n",
    "    # {}: Matches previous characters however many times specified within:\n",
    "# {n} : Exactly n times \n",
    "# {n,} : At least n times\n",
    "# {n,m} : Between n and m times\n",
    "\n",
    "pattern = 'ca*t' # itr will match 0 or more times the a between c and t so ct, cat, caaaaaat...\n",
    "\n",
    "pattern = 'aw{2}' # it gives us every time that aww appears because w have to be 2 times after an a\n",
    "\n",
    "\n",
    "   # CHARACTER CLASSES\n",
    "# \\w: Any alphanumeric character. equivalent to [A-Za-z0-9_]\n",
    "# \\W: Any non-alphanumeric character. equivalent to [^A-Za-z0-9_]\n",
    "# \\d: Any numeric character. [0-9]\n",
    "# \\D: Any non-numeric character. [^0-9]\n",
    "# \\s: Any whitespace characters. [ \\t\\n\\f]\n",
    "# \\S: Any non-whitespace characters.\n",
    "# \\\\b word boundry, but not use it much\n",
    "\n",
    "pattern = \"[\\w]+\" # it gives 1 or more alpha numerical characters together, so, complete words, without special characters: don't will be 'don', 't'\n",
    "pattern = \"[\\w']+\"  # in this case we will have \"don't\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "re.findall(pattern, text) \n",
    "\n",
    "text.find('string')  # this does not use regex, it gives you it's position\n",
    "\n",
    "list(re.finditer(r'[Mm]y | TPS |reports', text))  # it will print where they are each of them and the match \n",
    "\n",
    "\n",
    "re.sub(r'http\\S+', '', text1) # remove urls\n",
    "re.sub(r'\\d+','',text1) # remove numbers\n",
    "re.sub(r'\\W+',' ',text1).strip() # remove special characters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ordinal(df,col,names):\n",
    "    return df[col].map({names[0]:0, names[1]:1, names[2]:2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281e604",
   "metadata": {},
   "source": [
    "# Conecting MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep: import modules and get pwd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import getpass  # To get the password without showing the input\n",
    "password = getpass.getpass()\n",
    "\n",
    "connection_string = 'mysql+pymysql://root:' + password + '@localhost/bank'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "\n",
    "pd.read_sql(query,engine)       # in this case can be a query or a table, it accepts everything automatically\n",
    "pd.read_sql_table('table_name',engine)\n",
    "pd.read_sql_query(query,engine)       # create a query and then call it inside the read_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c510f9",
   "metadata": {},
   "source": [
    "# taking table from python to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass  # To get the password without showing the input\n",
    "password = getpass.getpass()\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "connection_string = 'mysql+pymysql://root:' + password + '@localhost/galicia_tourism'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "df_python.to_sql('table_sql', con = engine, if_exists = 'replace', index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e75711",
   "metadata": {},
   "source": [
    "# Where...check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] = np.where(df['col'].isin(['POJISTNE', 'SANKC. UROK', 'UVER']), 'other', df['col'] )\n",
    "\n",
    "\n",
    "np.where(data['GENDER'].isin(['J','C','A']) , 'U', data['GENDER'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b098c2",
   "metadata": {
    "id": "XKoP_4Xn7yBu"
   },
   "source": [
    "## Oversampling / Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d655603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling / undersampling is only to be done on the TRAINING set\n",
    "# our test set still must reflect reality!\n",
    "train = pd.concat([X_train_scaled, y_train],axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate majority/minority classes\n",
    "no_diabetes = train[train['Outcome']==0]\n",
    "yes_diabetes = train[train['Outcome']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample minority\n",
    "yes_diabetes_oversampled = resample(yes_diabetes, #<- sample from here\n",
    "                                    replace=True, #<- we need replacement, since we don't have enough data otherwise\n",
    "                                    n_samples = len(no_diabetes))#<- make both sets the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample majority\n",
    "no_diabetes_undersampled = resample(no_diabetes, #<- downsample from here\n",
    "                                    replace=False, #<- no need to reuse data now, we have an abundance\n",
    "                                    n_samples = len(yes_diabetes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecf461",
   "metadata": {
    "id": "GalHx4FP6DBv"
   },
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcfb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of magic, you can find documentation here: https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "# by deafult, takes a 5-neighbour KNN to build a new point, experimenting led us to choose 3 here\n",
    "sm = SMOTE(random_state=100, k_neighbors=3)\n",
    "X_train_SMOTE,y_train_SMOTE = sm.fit_resample(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb3548",
   "metadata": {},
   "source": [
    "# reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a696c475",
   "metadata": {},
   "source": [
    "# confidence interval and hyp test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6418a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can find a confidence interval for the population based on our sample\n",
    "import scipy.stats\n",
    "\n",
    "confidence_level = 0.95\n",
    "degrees_freedom = len(patients) - 1  # or, for large values of sample size, just use that\n",
    "sample_mean = np.mean(patients)\n",
    "# note that we use the standard error of the sample \n",
    "# as an estimate of the standard error of the population (which is used in the theoretical formula)\n",
    "sample_standard_error = scipy.stats.sem(patients) # sem = standard error of the mean = std(mean)/sqrt(samplesize)\n",
    "\n",
    "confidence_interval = scipy.stats.t.interval(confidence_level, \n",
    "                                             degrees_freedom, \n",
    "                                             sample_mean, \n",
    "                                             sample_standard_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# someone thinks (hypothesises) that the mean of \n",
    "# cholesterol values in the population is 5.6\n",
    "\n",
    "# we select a value for alpha of 0.05 (p-value threshold, significance level)\n",
    "# Two-sided test:\n",
    "# Null hypothesis or H0: mean cholesterol value = 5.6\n",
    "# Alternative hyp or H1: mean cholesterol value != 5.6 / <> 5.6\n",
    "\n",
    "\n",
    "# One-sided test:\n",
    "# Null hypothesis or H0: mean cholesterol value >= 5.6\n",
    "# Alternative hyp or H1: mean cholesterol value < 5.6\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "stat, pval = ttest_1samp(patients, 5.6)\n",
    "\n",
    "\n",
    "print('stat is  ', stat)\n",
    "print('pvalue for the two-tailed test is ', pval)\n",
    "\n",
    "print('pvalue for the one-tailed test is ', pval/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8f0ed",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "# Prob of | success in | trials, each trial has P_success of |\n",
    "#         v            v                                     v\n",
    "binom.pmf(3,           5,                                   0.5)\n",
    "# pmf = probability mass function  <-----\n",
    "# #P(x>1) = P(x=2)+P(x=3)...+P(x=5)\n",
    "P_more_than_1 = binom.pmf(2,5,0.5)+binom.pmf(3,5,0.5)+binom.pmf(4,5,0.5)+binom.pmf(5,5,0.5)\n",
    "P_more_than_1 = sum([binom.pmf(x,5,0.5) for x in range(2,21)])\n",
    "P_more_than_1\n",
    "# #P(x>1) = 1- ((P(x=0)+P(x=1))\n",
    "\n",
    "P_greater_than_one = 1 - sum([binom.pmf(x,5,.5) for x in range(2)])\n",
    "P_greater_than_one\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import geom\n",
    "# chance of needing | trials for first success, when P_success = |\n",
    "#                   v                                            v\n",
    "geom.pmf(           4,                                          1/6)\n",
    "# sum([geom.pmf(x, 1/3.5e06) for x in range(100000)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import uniform\n",
    "#      uniform between | and that +|    <---in continuous distributions we talk about cdf or pdf\n",
    "#                      v           v\n",
    "uniform.cdf(180,      0,          360)# cdf = continuous distribution fuction\n",
    "#              ^\n",
    "#give me P(x<= |)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "# P(x <= |), for normal dist with mean    | and stddev |)\n",
    "#        v                                v            v\n",
    "norm.cdf(110.5, 112, 9) - norm.cdf(109.5, 112,         9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03472c5",
   "metadata": {},
   "source": [
    "# 6.01 HTML - WebScraping - Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4962db6",
   "metadata": {},
   "source": [
    "# Respectful scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c24829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    wait_time = randint(1,4000)\n",
    "    print(\"I will sleep for \" + str(wait_time/1000) + \" seconds.\")\n",
    "    sleep(wait_time/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a94d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers = {\"Accept-Language\": \"en-US\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d64cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(presi_soups[-1][0].select(\"span.bday\")[0].get_text())\n",
    "\n",
    "# #Political party\n",
    "\n",
    "presi_soups[-1][0].find(\"th\", string = \"Political party\").parent.select('a')[0].get_text()\n",
    "\n",
    "\n",
    "# # #Number of sons/daughters\n",
    "len(presi_soups[-1][0].find(\"th\", string=\"Children\").parent.find_all(\"li\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6901619",
   "metadata": {},
   "source": [
    "# Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f95541",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df:\n",
    "    try:\n",
    "        print(whatever)\n",
    "    except:\n",
    "        print(\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138766ae",
   "metadata": {},
   "source": [
    "# Taking out nested columns 6.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6155c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(data, col_list):\n",
    "    for column in col_list:\n",
    "        flattened = pd.DataFrame(dict(data[column])).transpose()\n",
    "        columns = [str(col) for col in flattened.columns]\n",
    "        flattened.columns = [column + '_' + colname for colname in columns]\n",
    "        data = pd.concat([data, flattened], axis=1)\n",
    "        data = data.drop(column, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ab48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize  # pandas do it for itself\n",
    "\n",
    "results = response.json()\n",
    "flattened_data = json_normalize(results)\n",
    "\n",
    "flattened_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fbded",
   "metadata": {},
   "source": [
    "# Spotipy 6.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spotipy\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "#Initialize SpotiPy with user credentials\n",
    "#sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id='<your client id here>',\n",
    "#                                                           client_secret='<your client secret here>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = sp.search(q='Lady Gaga', limit=50)\n",
    "\n",
    "playlist = sp.user_playlist_tracks(\"spotify\", \"3hAI8gTEHJcPOI3bKq7Gx4\")\n",
    "\n",
    "results = sp.search(q='artist:Rolling Stones', type='track', limit=10)\n",
    "\n",
    "next_page = sp.next(playlist)\n",
    "\n",
    "# get the audio features for that song\n",
    "sp.audio_features(song_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc85cf",
   "metadata": {},
   "source": [
    "# Flatten -> a list of lists 6.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattening is not in the standard library but used a lot. you'll thank me later\n",
    "def flatten(input_list):\n",
    "    return [item for sublist in input_list for item in sublist]\n",
    "\n",
    "# outlist=[]\n",
    "# for sublist in input_list:\n",
    "#     for item in sublist:\n",
    "#         outlist.append(item)\n",
    "# return outlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd3d86",
   "metadata": {},
   "source": [
    "# Unsupervised machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341bbce",
   "metadata": {},
   "source": [
    "# K-Means Clusters 6.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# (need to be scaled)\n",
    "\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "pred = kmeans.predict(X)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=pred,alpha=0.5)  # c=colour\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking which is the best amount of clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "K = range(2, 20)\n",
    "inertia = []\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k,\n",
    "                    random_state=1234)\n",
    "    kmeans.fit(X_prep)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, inertia, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(np.arange(min(K), max(K)+1, 1.0))\n",
    "plt.title('Elbow Method showing the optimal k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83287541",
   "metadata": {},
   "source": [
    "# Agglomerative clustering 6.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward linkage tends to produce relatively equally sized clusters\n",
    "agglomerative = cluster.AgglomerativeClustering(n_clusters=3,linkage='ward') # linkage = when we decide to link the clusters\n",
    "pred = agglomerative.fit_predict(X)                                          # 'ward' 'complete' 'average' 'single'\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=pred,alpha=0.5)\n",
    "#plt.scatter(X['alcohol'], X['malic_acid'], c=pred,alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# this cannot be used for latter using a function to select a cluster for new values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa1a86",
   "metadata": {},
   "source": [
    "# PCA 6.07       -->   eliminate multicolinearity sacrificing explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2dd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "my_pca = PCA()\n",
    "my_pca.fit(X_train_scaled)\n",
    "\n",
    "pca = PCA(7)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "pca = PCA(0.90)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "pca.explained_variance_ratio_[:7].sum()\n",
    "\n",
    "plt.scatter(X_train_pca[:,0],X_train_pca[:,1], c = y_train) # visualization might help a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819de2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "#cum_sum_eigenvalues\n",
    "\n",
    "# Create the visualization plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, \n",
    "        alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, \n",
    "         where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb578ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of how the components of pca mixes and are composed by the columns that where taken from\n",
    "# they are factors\n",
    "\n",
    "plt.matshow(pca.components_[0:6], cmap='coolwarm')\n",
    "plt.yticks([0,1,2,3,4,5],['1st Comp','2nd Comp','3rd Comp','4th','5th','6th'],fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(X.columns)),X.columns,rotation=65,ha='left')\n",
    "plt.tight_layout()\n",
    "plt.show()# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7314cc",
   "metadata": {},
   "source": [
    "# Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cdf5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437a681",
   "metadata": {},
   "source": [
    "# Low Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e8fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to look at VarianceThresholds we need all the variable to be on the same scale\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold \n",
    "var_threshold = 0.02\n",
    "sel = VarianceThreshold(threshold=(var_threshold))\n",
    "\n",
    "# 1) default is threshold = 0 :eliminate all coumns that are identical for all rows\n",
    "# 2) in practice we would scale the columns first, and then apply threshold, or apply \n",
    "#    a different threshold for different columns\n",
    "\n",
    "# This drops the columns that have a variance less than this threshold\n",
    "sel = sel.fit(numerical_scaled)\n",
    "temp = sel.transform(numerical_scaled)\n",
    "temp = pd.DataFrame(temp)\n",
    "print(numerical_scaled.shape)\n",
    "print(temp.shape)\n",
    "\n",
    "\n",
    "# To check which columns were removed, you can manipulate the results\n",
    "# from the code below. This gives the variance of each feature in order\n",
    "# of appearance of the dataset.\n",
    "sel.variances_\n",
    "\n",
    "\n",
    "\n",
    "#This gives you the result as True and False for the columns that we\n",
    "# selected and those which were removed, respectively.\n",
    "sel.variances_ > var_threshold\n",
    "sel.get_support()\n",
    "var_list = list(sel.get_support())\n",
    "var_list\n",
    "\n",
    "list(zip(numerical.columns, var_list))\n",
    "\n",
    "[col[0] for col in zip(numerical.columns, var_list) if col[1] == False]\n",
    "\n",
    "\n",
    "\n",
    "# So these are the columns we can drop\n",
    "# if we also want to see the actual variances we can use this:\n",
    "removed_columns = pd.DataFrame(data=(numerical.columns,sel.variances_,sel.get_support()), index=('column_name','variance','statement')).T\n",
    "removed_columns.loc[(removed_columns['statement'] == False),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b305763",
   "metadata": {},
   "source": [
    "# ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e242313",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(list_1, list_2)) # it takes the 1st element of 1st list + 1st element of 2nd list and make a tupple\n",
    "\n",
    "# this makes a list from other 2 lists\n",
    "# allows us to go trhou 2 lists together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53512c11",
   "metadata": {},
   "source": [
    "# Chi 2 & KBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4952b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "kbest = SelectKBest(chi2, k=10).fit_transform(X, y)\n",
    "\n",
    "# To check the scores\n",
    "model = SelectKBest(chi2, k=10).fit(X, y)\n",
    "df = pd.DataFrame(data = model.scores_, columns = ['score'])\n",
    "df['Column'] = numerical.columns\n",
    "print(df.sort_values(by = ['score'], ascending = False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597eec7c",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd34181",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numerical_scaled\n",
    "y = targets['TARGET_D']\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import linear_model\n",
    "lm = linear_model.LinearRegression()\n",
    "rfe = RFE(lm, n_features_to_select=20, verbose=False)  # n_features_to_select=20 number of features we want to end up with\n",
    "rfe.fit(X, y)                                          #  verbose=True it gives information of each feature\n",
    "\n",
    "\n",
    "rfe.ranking_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424d3b1",
   "metadata": {},
   "source": [
    "# Hypothesis testing with paired samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00df07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same results, different code\n",
    "\n",
    "\n",
    "st.ttest_rel(blood_pressure['after'], blood_pressure['before'])\n",
    "\n",
    "\n",
    "st.ttest_1samp(blood_pressure['after'] - blood_pressure['before'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de376b8",
   "metadata": {},
   "source": [
    "# Pooled samples / independent samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb512fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independent Samples\n",
    "#For two groups where we cannot match the observations to one another. \n",
    "#In this case transactions from a website with different interfaces (a, b)\n",
    "\n",
    "\n",
    "st.ttest_ind(ab_test['a'], ab_test['b'], equal_var=False) # if we don't assume equal variance the test will be more robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2ad37",
   "metadata": {},
   "source": [
    "# ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8baabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0 for ANOVA is always that the means of the various groups are the same\n",
    "# H1 is that they are not the same\n",
    "st.f_oneway(interest_r_pivot['City_1'],interest_r_pivot.City_2,interest_r_pivot.City_3,interest_r_pivot.City_4,interest_r_pivot.City_5,interest_r_pivot.City_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2542fd",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8be3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=4) # for any decission the max number of questions must be 4\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"test data accuracy was \",model.score(X_test,y_test))\n",
    "# 100% on training data\n",
    "print(\"train data accuracy was \",model.score(X_train,y_train))\n",
    "\n",
    "\n",
    "model.predict(X_test)\n",
    "\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394c6ae",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regr = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "model = regr.fit(X_train, y_train)\n",
    "\n",
    "print(\"test data score was: \",regr.score(X_test, y_test))\n",
    "print(\"train data score was: \",regr.score(X_train, y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e065da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "r = export_text(regr, feature_names=list(features.columns))\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449fe5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(regr, feature_names=boston.feature_names, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265cba0",
   "metadata": {},
   "source": [
    "# Decision Trees checking for depths \n",
    "#### (it can handle nans and doesn't need scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why dont I repeat the process but choose the max_depth of the tree\n",
    "max_depth = range(1,30)\n",
    "test = []\n",
    "train = []\n",
    "\n",
    "for depth in max_depth:\n",
    "    model = DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    test.append(model.score(X_test,y_test))\n",
    "    train.append(model.score(X_train,y_train))\n",
    "    \n",
    "    \n",
    "    \n",
    "# we quickly see overfitting properties\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot( train, label=\"training accuracy\")\n",
    "plt.plot( test, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_depth-1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code helps you plot the feature importance!\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 12), dpi=80)\n",
    "\n",
    "import numpy as np\n",
    "def plot_feature_importances(model, namelist):\n",
    "    n_features = len(model.feature_importances_)\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), namelist)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "\n",
    "plot_feature_importances(model, cancer.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33762734",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a23354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "results = cross_validate(regr,X_train, y_train, cv = 5)\n",
    "\n",
    "print(results['test_score'])\n",
    "print(results['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63bfb0",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7a95f",
   "metadata": {},
   "source": [
    "## 1. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "max_depth_choices= [3,10,None]\n",
    "criterion_choices = ['mse','mae']\n",
    "min_samples_split_choices = [2,10]\n",
    "min_samples_leaf_choices = [2,10]   \n",
    "\n",
    "\n",
    "# Create the  grid \n",
    "# this is a dictionary from hyperparameters to potential values\n",
    "# the keys in this dictionary have to match the names of the hyperparameters in the documentation of the model\n",
    "grid = {'max_depth': max_depth_choices,\n",
    "        'criterion': criterion_choices,\n",
    "        'min_samples_split': min_samples_split_choices,\n",
    "        'min_samples_leaf': min_samples_leaf_choices}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model object\n",
    "\n",
    "# estimator -> model to optimize \n",
    "model = DecisionTreeRegressor()\n",
    "# param_grid -> state the dictionary of parameters to optimize\n",
    "# cv = 5 -> number of cross validation folds <------ CV is REALLY important in grid search. Why?\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = grid, cv = 5)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# and the winner is...\n",
    "grid_search.best_params_\n",
    "\n",
    "\n",
    "# in grid search you are more likely to get really good results in your training set, even with CV\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ef8e5",
   "metadata": {},
   "source": [
    "## 2. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "max_depth_choices= [3,4,5,6,7,8,9,10,None]\n",
    "criterion_choices = ['mse','mae']\n",
    "min_samples_split_choices = [2,3,4,5,6,7,8,9,10]\n",
    "min_samples_leaf_choices = [2,3,4,5,6,7,8,9,10]\n",
    "max_features_choices = [2,3,4,5,6]\n",
    "\n",
    "\n",
    "\n",
    "random_grid = {'max_depth': max_depth_choices,\n",
    "               'criterion': criterion_choices,\n",
    "               'min_samples_split': min_samples_split_choices,\n",
    "               'min_samples_leaf': min_samples_leaf_choices,\n",
    "               'max_features': max_features_choices}\n",
    "\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "random_search = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter=25, cv = 5, n_jobs = 10)\n",
    "\n",
    "\n",
    "random_search.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "random_search.best_params_\n",
    "\n",
    "\n",
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d1373",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=5, # max number of questions\n",
    "                             min_samples_split=20, # amount of rows still considered at every question\n",
    "                             min_samples_leaf =20,  # ultimate answer based on at least this many rows\n",
    "                             max_samples=0.8,  # fractrion of X-train to use in each tree\n",
    "                             random_state = 42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "display(y_test.value_counts())\n",
    "display(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5444885",
   "metadata": {},
   "source": [
    "# EXCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7324a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "\n",
    "wb = Workbook() # creating a new excel file in python memory\n",
    "\n",
    "wb.create_sheet(\"Mysheet1\")\n",
    "\n",
    "wb.remove(wb['Sheet'])\n",
    "\n",
    "wb.sheetnames\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "load_workbook('L. 8.01_exercise.xlsx')\n",
    "\n",
    "wb.save(\"sample.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ae3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in regions.iter_rows(min_row=1, max_row=4, min_col=1, max_col=4):\n",
    "  print('changing row...')\n",
    "  for cell in row:\n",
    "    print(cell.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04e709",
   "metadata": {},
   "source": [
    "#  NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac25a3b",
   "metadata": {},
   "source": [
    "#  NPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(text)\n",
    "[word for word in tokens if word.isalnum()]\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(new_text)\n",
    "\n",
    "\n",
    "# Part of speech can be a useful feature in itself, but is also heavily used in making lemmatization and stemming more effective\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# stemming can be done as cleaning technique -> treats prefixes and suffixes.\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stemmed = [ps.stem(w) for w in tokens]\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "[word for word in lemmatized if not word in stopwords.words()]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vect = CountVectorizer()\n",
    "# fit creates one entry for each different word seen  \n",
    "bow_vect.fit([\" \".join(without_sw)])\n",
    "sorted(bow_vect.vocabulary_, key=bow_vect.vocabulary_.get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
